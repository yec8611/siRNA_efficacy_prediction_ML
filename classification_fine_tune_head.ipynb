{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from multimolecule import RnaTokenizer, RnaFmModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import gc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df = df.drop('inhibition_value', axis=1)\n",
    "df = df.rename(columns={'label_cls': 'inhibition'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"multimolecule/rnafm\"\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 8\n",
    "NUM_FOLDS = 5\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = RnaTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_token = tokenizer.sep_token if tokenizer.sep_token else \"[SEP]\"\n",
    "def combine_sequences(sirna, mrna):\n",
    "    sirna_str = str(sirna)\n",
    "    mrna_str = str(mrna)\n",
    "    return f\"{sirna_str}{sep_token}{mrna_str}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['combined_sequence'] = df.apply(lambda row: combine_sequences(row['siRNA_sequence'], row['mRNA_sequence']), axis=1)\n",
    "print(\"\\nData with combined sequences:\")\n",
    "print(df[['combined_sequence', 'inhibition']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiRnaClassificationDataset(Dataset):\n",
    "    def __init__(self, sequences, targets, tokenizer, max_len):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = str(self.sequences[idx])\n",
    "        target = int(self.targets[idx])\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sequence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(target, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_sequences = df['combined_sequence'].tolist()\n",
    "all_labels = df['inhibition'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "# def cls_pooling(model_output, attention_mask=None):\n",
    "#     return model_output.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pooling_strategy = mean_pooling # Choose mean_pooling or cls_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_classifier(input_size):\n",
    "    return nn.Linear(input_size, 1)\n",
    "\n",
    "def create_medium_mlp_classifier(input_size, hidden_dim=256):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_size, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(hidden_dim, 1)\n",
    "    )\n",
    "\n",
    "def create_deep_mlp_classifier(input_size, hidden_dims=[512, 256, 128]):\n",
    "    layers = []\n",
    "    current_dim = input_size\n",
    "    for h_dim in hidden_dims:\n",
    "        layers.append(nn.Linear(current_dim, h_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(0.1))\n",
    "        current_dim = h_dim\n",
    "    layers.append(nn.Linear(current_dim, 1))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnaFmForSequenceClassification(nn.Module):\n",
    "    def __init__(self, model_name, head_type='medium', pooling_func=mean_pooling):\n",
    "        super().__init__()\n",
    "        print(f\"Loading base model: {model_name}\")\n",
    "        self.rna_fm = RnaFmModel.from_pretrained(model_name)\n",
    "        hidden_size = self.rna_fm.config.hidden_size\n",
    "        self.pooling = pooling_func\n",
    "\n",
    "        print(\"Freezing base model parameters...\")\n",
    "        num_frozen = 0\n",
    "        for param in self.rna_fm.parameters():\n",
    "            param.requires_grad = False\n",
    "            num_frozen += 1\n",
    "        print(f\"Froze {num_frozen} parameter tensors in the base model.\")\n",
    "\n",
    "        print(f\"Creating classification head of type: {head_type}\")\n",
    "        if head_type == 'simple':\n",
    "            self.classifier = create_simple_classifier(hidden_size)\n",
    "        elif head_type == 'medium':\n",
    "            self.classifier = create_medium_mlp_classifier(hidden_size)\n",
    "        elif head_type == 'deep':\n",
    "            self.classifier = create_deep_mlp_classifier(hidden_size)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid head_type. Choose 'simple', 'medium', or 'deep'.\")\n",
    "\n",
    "        print(\"Model initialization complete.\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.rna_fm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        pooled_output = self.pooling(outputs, attention_mask)\n",
    "\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_and_optimizer(model_name, head_type, learning_rate, weight_decay):\n",
    "    model = RnaFmForSequenceClassification(model_name, head_type=head_type, pooling_func=pooling_strategy)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    print(\"\\nOptimizer initialized. Training parameters:\")\n",
    "    num_trainable = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"- {name} ({param.numel()})\")\n",
    "            num_trainable += param.numel()\n",
    "    print(f\"Total trainable parameters: {num_trainable}\")\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    return model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "EPOCHS = 10\n",
    "HEAD_TYPE = 'medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"  Train Epoch completed in {elapsed_time:.2f}s, Avg. Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "            predictions = (probabilities > 0.5).int()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_probabilities = np.array(all_probabilities)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='binary', zero_division=0)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(all_labels, all_probabilities)\n",
    "    except ValueError:\n",
    "        roc_auc = float('nan')\n",
    "        print(\"  Warning: ROC AUC could not be calculated (likely only one class in validation set).\")\n",
    "\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"  Evaluation completed in {elapsed_time:.2f}s\")\n",
    "    return avg_loss, accuracy, precision, recall, f1, roc_auc, all_labels, all_predictions\n",
    "\n",
    "\n",
    "def run_training_fold(model, optimizer, criterion, train_loader, val_loader, epochs, device, fold_num):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    metrics_history = []\n",
    "\n",
    "    print(f\"Starting training for Fold {fold_num}, {epochs} epochs...\")\n",
    "    total_fold_start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f\"\\n-- Epoch {epoch+1}/{epochs} --\")\n",
    "\n",
    "        avg_train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        avg_val_loss, val_accuracy, val_precision, val_recall, val_f1, val_roc_auc, _, _ = evaluate_model(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        metrics_history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'val_precision': val_precision,\n",
    "            'val_recall': val_recall,\n",
    "            'val_f1': val_f1,\n",
    "            'val_roc_auc': val_roc_auc\n",
    "        })\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch+1} Summary | Time: {epoch_time:.2f}s\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.4f} | Val F1: {val_f1:.4f} | Val AUC: {val_roc_auc:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            print(f\"  -> New best validation loss: {best_val_loss:.4f}. Saving model state.\")\n",
    "\n",
    "        if device == torch.device(\"mps\"):\n",
    "             torch.mps.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    total_fold_time = time.time() - total_fold_start_time\n",
    "    print(f\"\\nTraining finished for Fold {fold_num}. Total time: {total_fold_time:.2f}s\")\n",
    "\n",
    "    return best_model_state, train_losses, val_losses, metrics_history, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=SEED)\n",
    "fold_results = []\n",
    "all_metrics_history = []\n",
    "\n",
    "full_dataset = SiRnaClassificationDataset(all_sequences, all_labels, tokenizer, MAX_LENGTH)\n",
    "\n",
    "print(f\"\\n--- Starting {NUM_FOLDS}-Fold Cross-Validation ---\")\n",
    "cv_start_time = time.time()\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(all_sequences)):\n",
    "    fold_num = fold + 1\n",
    "    print(f\"\\n==================== Fold {fold_num}/{NUM_FOLDS} ====================\")\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "    train_loader = DataLoader(full_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, num_workers=0)\n",
    "    val_loader = DataLoader(full_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, num_workers=0)\n",
    "\n",
    "    print(f\"Fold {fold_num}: Train samples = {len(train_idx)}, Validation samples = {len(val_idx)}\")\n",
    "\n",
    "    model, optimizer, criterion = initialize_model_and_optimizer(\n",
    "        MODEL_NAME, HEAD_TYPE, LEARNING_RATE, WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "    best_model_state, train_losses, val_losses, metrics_history, fold_best_val_loss = run_training_fold(\n",
    "        model, optimizer, criterion, train_loader, val_loader, EPOCHS, device, fold_num\n",
    "    )\n",
    "    all_metrics_history.append(metrics_history)\n",
    "\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"\\nLoaded best model state from training for final evaluation.\")\n",
    "    else:\n",
    "        print(\"\\nWarning: No best model state saved (check training). Evaluating last state.\")\n",
    "\n",
    "    print(f\"Performing final evaluation for Fold {fold_num}...\")\n",
    "    final_val_loss, final_accuracy, final_precision, final_recall, final_f1, final_roc_auc, fold_labels, fold_preds = evaluate_model(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "\n",
    "    print(f\"\\nFold {fold_num} Final Validation Metrics (Best Model):\")\n",
    "    print(f\"  Loss:      {final_val_loss:.4f}\")\n",
    "    print(f\"  Accuracy:  {final_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {final_precision:.4f}\")\n",
    "    print(f\"  Recall:    {final_recall:.4f}\")\n",
    "    print(f\"  F1 Score:  {final_f1:.4f}\")\n",
    "    print(f\"  ROC AUC:   {final_roc_auc:.4f}\")\n",
    "\n",
    "    fold_results.append({\n",
    "        'fold': fold_num,\n",
    "        'accuracy': final_accuracy,\n",
    "        'precision': final_precision,\n",
    "        'recall': final_recall,\n",
    "        'f1': final_f1,\n",
    "        'roc_auc': final_roc_auc,\n",
    "        'best_val_loss': fold_best_val_loss,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "    })\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, EPOCHS + 1), train_losses, label='Training Loss', marker='o')\n",
    "    plt.plot(range(1, EPOCHS + 1), val_losses, label='Validation Loss', marker='x')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (BCEWithLogits)')\n",
    "    plt.title(f'Fold {fold_num} - Training & Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    epochs_list = [m['epoch'] for m in metrics_history]\n",
    "    f1_list = [m['val_f1'] for m in metrics_history]\n",
    "    auc_list = [m['val_roc_auc'] for m in metrics_history]\n",
    "    plt.plot(epochs_list, f1_list, label='Validation F1 Score', marker='s')\n",
    "    plt.plot(epochs_list, auc_list, label='Validation ROC AUC', marker='^')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.title(f'Fold {fold_num} - Validation F1 & ROC AUC')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    try:\n",
    "        cm = confusion_matrix(fold_labels, fold_preds)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f'Fold {fold_num} - Confusion Matrix (Best Model)')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not display confusion matrix for Fold {fold_num}: {e}\")\n",
    "\n",
    "    print(f\"Cleaning up Fold {fold_num} resources...\")\n",
    "    del model, optimizer, criterion, train_loader, val_loader, train_sampler, val_sampler, best_model_state\n",
    "    if device == torch.device(\"mps\"):\n",
    "        torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "cv_end_time = time.time()\n",
    "print(f\"\\n--- Cross-Validation Finished --- Total Time: {cv_end_time - cv_start_time:.2f}s ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_df = pd.DataFrame(fold_results)\n",
    "\n",
    "print(\"\\n--- Cross-Validation Summary ---\")\n",
    "\n",
    "print(results_df[[\n",
    "    'fold', 'accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'best_val_loss'\n",
    "]].round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nAverage Metrics Across Folds:\")\n",
    "avg_accuracy = results_df['accuracy'].mean()\n",
    "std_accuracy = results_df['accuracy'].std()\n",
    "avg_precision = results_df['precision'].mean()\n",
    "std_precision = results_df['precision'].std()\n",
    "avg_recall = results_df['recall'].mean()\n",
    "std_recall = results_df['recall'].std()\n",
    "avg_f1 = results_df['f1'].mean()\n",
    "std_f1 = results_df['f1'].std()\n",
    "avg_roc_auc = results_df['roc_auc'].mean()\n",
    "std_roc_auc = results_df['roc_auc'].std()\n",
    "\n",
    "print(f\"  Accuracy:  {avg_accuracy:.4f} +/- {std_accuracy:.4f}\")\n",
    "print(f\"  Precision: {avg_precision:.4f} +/- {std_precision:.4f}\")\n",
    "print(f\"  Recall:    {avg_recall:.4f} +/- {std_recall:.4f}\")\n",
    "print(f\"  F1 Score:  {avg_f1:.4f} +/- {std_f1:.4f}\")\n",
    "print(f\"  ROC AUC:   {avg_roc_auc:.4f} +/- {std_roc_auc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
