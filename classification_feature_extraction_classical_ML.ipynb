{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from multimolecule import RnaTokenizer, RnaFmModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon GPU) for feature extraction.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA for feature extraction.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU for feature extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "# data.csv has four columns, siRNA_sequence, mRNA_sequence, inhibition_value, label_cls. inhibition_value has continuous values 0-1, label_cls is 0 or 1\n",
    "df = df.drop('inhibition_value', axis=1)\n",
    "df = df.rename(columns={'label_cls': 'inhibition'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_NAME = \"multimolecule/rnafm\"\n",
    "MAX_LENGTH = 512\n",
    "EXTRACT_BATCH_SIZE = 16 \n",
    "NUM_FOLDS = 5\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = RnaTokenizer.from_pretrained(MODEL_NAME)\n",
    "sep_token = tokenizer.sep_token if tokenizer.sep_token else \"[SEP]\"\n",
    "\n",
    "def combine_sequences(sirna, mrna):\n",
    "    sirna_str = str(sirna)\n",
    "    mrna_str = str(mrna)\n",
    "    return f\"{sirna_str}{sep_token}{mrna_str}\"\n",
    "\n",
    "df['combined_sequence'] = df.apply(lambda row: combine_sequences(row['siRNA_sequence'], row['mRNA_sequence']), axis=1)\n",
    "all_sequences = df['combined_sequence'].tolist()\n",
    "\n",
    "all_labels = df['inhibition'].to_numpy(dtype=int)\n",
    "\n",
    "print(f\"\\nPrepared {len(all_sequences)} sequences for feature extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "class ExtractionDataset(Dataset):\n",
    "    def __init__(self, sequences, tokenizer, max_len):\n",
    "        self.sequences = sequences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self): return len(self.sequences)\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = str(self.sequences[idx])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sequence, add_special_tokens=True, max_length=self.max_len,\n",
    "            return_token_type_ids=False, padding='max_length', truncation=True,\n",
    "            return_attention_mask=True, return_tensors='pt',\n",
    "        )\n",
    "        return {'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten()}\n",
    "\n",
    "\n",
    "def extract_rnafm_features(sequences, model, tokenizer, max_len, batch_size, device):\n",
    "    dataset = ExtractionDataset(sequences, tokenizer, max_len)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, sampler=SequentialSampler(dataset))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    all_features = []\n",
    "    print(f\"Starting feature extraction for {len(sequences)} sequences on device: {device}\")\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "            pooled_output = mean_pooling(outputs, attention_mask)\n",
    "            all_features.append(pooled_output.cpu().numpy())\n",
    "            if (i + 1) % 50 == 0:\n",
    "                 elapsed = time.time() - start_time\n",
    "                 print(f\"  Processed batch {i+1}/{len(data_loader)} ({elapsed:.2f}s)\")\n",
    "    print(f\"Feature extraction completed in {time.time() - start_time:.2f}s\")\n",
    "    features_array = np.concatenate(all_features, axis=0)\n",
    "    print(f\"Extracted features shape: {features_array.shape}\") \n",
    "    return features_array\n",
    "\n",
    "\n",
    "print(f\"\\nLoading RNA-FM model: {MODEL_NAME}\")\n",
    "\n",
    "try:\n",
    "    rnafm_model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "except Exception:\n",
    "    print(\"AutoModel failed, trying RnaFmModel directly...\")\n",
    "    rnafm_model = RnaFmModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "\n",
    "X_features = extract_rnafm_features(\n",
    "    all_sequences,\n",
    "    rnafm_model,\n",
    "    tokenizer,\n",
    "    MAX_LENGTH,\n",
    "    EXTRACT_BATCH_SIZE,\n",
    "    device\n",
    ")\n",
    "y_labels = all_labels # Our target variable (0 or 1)\n",
    "\n",
    "\n",
    "del rnafm_model\n",
    "if device == torch.device(\"mps\"): torch.mps.empty_cache()\n",
    "elif device == torch.device(\"cuda\"): torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"RNA-FM model removed from memory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_count = np.sum(y_labels == 0)\n",
    "pos_count = np.sum(y_labels == 1)\n",
    "scale_pos_weight_val = neg_count / pos_count if pos_count > 0 else 1\n",
    "print(f\"Calculated scale_pos_weight for XGBoost: {scale_pos_weight_val:.2f}\")\n",
    "\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=200,       \n",
    "        max_depth=20,           \n",
    "        min_samples_split=5,    \n",
    "        min_samples_leaf=2,     \n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1,              \n",
    "        random_state=SEED\n",
    "    ),\n",
    "\n",
    "    \"XGBoost\": xgb.XGBClassifier(\n",
    "        n_estimators=300,       \n",
    "        learning_rate=0.1,      \n",
    "        max_depth=5,            \n",
    "        subsample=0.8,          \n",
    "        colsample_bytree=0.8,   \n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',  \n",
    "        use_label_encoder=False,\n",
    "        scale_pos_weight=scale_pos_weight_val, \n",
    "        n_jobs=-1,\n",
    "        random_state=SEED,\n",
    "    ),\n",
    "\n",
    "    \"SVC_RBF\": SVC(\n",
    "        kernel='rbf',\n",
    "        C=10.0,                 \n",
    "        gamma='scale',          \n",
    "        class_weight='balanced',\n",
    "        probability=True,       \n",
    "        random_state=SEED\n",
    "    ),\n",
    "\n",
    "    \"LightGBM\": lgb.LGBMClassifier(\n",
    "        n_estimators=300,       \n",
    "        learning_rate=0.1,      \n",
    "        max_depth=7,            \n",
    "        num_leaves=31,          \n",
    "        subsample=0.8,          \n",
    "        colsample_bytree=0.8,   \n",
    "        objective='binary',\n",
    "        metric='auc',           \n",
    "        scale_pos_weight=scale_pos_weight_val, \n",
    "        n_jobs=-1,\n",
    "        random_state=SEED\n",
    "    ),\n",
    "\n",
    "    \"KNeighbors\": KNeighborsClassifier(\n",
    "        n_neighbors=7,          \n",
    "        weights='distance',     \n",
    "        p=2                     \n",
    "    ),\n",
    "\n",
    "     \"LogisticRegression\": LogisticRegression(\n",
    "         C=1.0,                 \n",
    "         penalty='l2',          \n",
    "         solver='liblinear',    \n",
    "         class_weight='balanced',\n",
    "         max_iter=1000,         \n",
    "         random_state=SEED\n",
    "     )\n",
    "}\n",
    "\n",
    "print(f\"\\nDefined {len(models)} classical ML classifiers for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=SEED)\n",
    "cv_results = []\n",
    "\n",
    "print(f\"\\n--- Starting {NUM_FOLDS}-Fold Cross-Validation for Classical Classifiers ---\")\n",
    "\n",
    "X = X_features\n",
    "y = y_labels\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "    fold_num = fold + 1\n",
    "    print(f\"\\n--- Fold {fold_num}/{NUM_FOLDS} ---\")\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    for model_name, model_instance in models.items():\n",
    "        print(f\"  Training {model_name}...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', model_instance)\n",
    "        ])\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        try:\n",
    "            y_prob = pipeline.predict_proba(X_val)[:, 1]\n",
    "        except AttributeError:\n",
    "            print(f\"    Note: {model_name} does not have predict_proba. ROC AUC calculated using decision function or predict.\")\n",
    "            try:\n",
    "                 y_scores = pipeline.decision_function(X_val)\n",
    "                 if y_scores.ndim > 1 and y_scores.shape[1] > 1:\n",
    "                     y_prob = y_scores[:, 1]\n",
    "                 else:\n",
    "                     y_prob = y_scores\n",
    "            except AttributeError:\n",
    "                 y_prob = y_pred\n",
    "\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, average='binary', zero_division=0)\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_val, y_prob)\n",
    "        except ValueError:\n",
    "            roc_auc = float('nan')\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"    {model_name} Fold {fold_num} | Acc: {accuracy:.4f} | F1: {f1:.4f} | AUC: {roc_auc:.4f} | Time: {elapsed:.2f}s\")\n",
    "\n",
    "        cv_results.append({\n",
    "            'fold': fold_num,\n",
    "            'model': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'roc_auc': roc_auc\n",
    "        })\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n--- Cross-Validation Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(cv_results)\n",
    "\n",
    "print(\"\\n--- Cross-Validation Summary (Classical Classifiers on RNA-FM Features) ---\")\n",
    "\n",
    "summary = results_df.groupby('model').agg(\n",
    "    avg_acc=('accuracy', 'mean'), std_acc=('accuracy', 'std'),\n",
    "    avg_prec=('precision', 'mean'), std_prec=('precision', 'std'),\n",
    "    avg_rec=('recall', 'mean'), std_rec=('recall', 'std'),\n",
    "    avg_f1=('f1', 'mean'), std_f1=('f1', 'std'),\n",
    "    avg_auc=('roc_auc', 'mean'), std_auc=('roc_auc', 'std')\n",
    ").reset_index()\n",
    "\n",
    "summary['Accuracy'] = summary.apply(lambda row: f\"{row['avg_acc']:.4f} +/- {row['std_acc']:.4f}\", axis=1)\n",
    "summary['Precision'] = summary.apply(lambda row: f\"{row['avg_prec']:.4f} +/- {row['std_prec']:.4f}\", axis=1)\n",
    "summary['Recall'] = summary.apply(lambda row: f\"{row['avg_rec']:.4f} +/- {row['std_rec']:.4f}\", axis=1)\n",
    "summary['F1 Score'] = summary.apply(lambda row: f\"{row['avg_f1']:.4f} +/- {row['std_f1']:.4f}\", axis=1)\n",
    "summary['ROC AUC'] = summary.apply(lambda row: f\"{row['avg_auc']:.4f} +/- {row['std_auc']:.4f}\", axis=1)\n",
    "\n",
    "print(summary[['model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']].to_string(index=False))\n",
    "\n",
    "best_model_f1 = summary.loc[summary['avg_f1'].idxmax()]\n",
    "print(f\"\\nBest model based on average F1 Score: {best_model_f1['model']} (F1 = {best_model_f1['F1 Score']})\")\n",
    "best_model_auc = summary.loc[summary['avg_auc'].idxmax()]\n",
    "print(f\"Best model based on average ROC AUC:  {best_model_auc['model']} (AUC = {best_model_auc['ROC AUC']})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
