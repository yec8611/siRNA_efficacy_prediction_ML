{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from multimolecule import RnaTokenizer, RnaFmModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"multimolecule/rnafm\"\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 8\n",
    "NUM_FOLDS = 5\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RnaTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_token = tokenizer.sep_token if tokenizer.sep_token else \"[SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sequences(sirna, mrna):\n",
    "    sirna_str = str(sirna)\n",
    "    mrna_str = str(mrna)\n",
    "    return f\"{sirna_str}{sep_token}{mrna_str}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['combined_sequence'] = df.apply(lambda row: combine_sequences(row['siRNA_sequence'], row['mRNA_sequence']), axis=1)\n",
    "print(\"\\nData with combined sequences:\")\n",
    "print(df[['combined_sequence', 'inhibition_value']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiRnaDataset(Dataset):\n",
    "    def __init__(self, sequences, targets, tokenizer, max_len):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = str(self.sequences[idx])\n",
    "        target = float(self.targets[idx])\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sequence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(target, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "all_sequences = df['combined_sequence'].tolist()\n",
    "all_labels = df['inhibition_value'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "def create_simple_regressor(input_size):\n",
    "    return nn.Linear(input_size, 1)\n",
    "\n",
    "def create_medium_mlp_head(input_size, hidden_dim=256):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_size, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.1),\n",
    "        nn.Linear(hidden_dim, 1)\n",
    "    )\n",
    "\n",
    "def create_deep_mlp_head(input_size, hidden_dims=[512, 256, 128]):\n",
    "    layers = []\n",
    "    current_dim = input_size\n",
    "    for h_dim in hidden_dims:\n",
    "        layers.append(nn.Linear(current_dim, h_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(0.1))\n",
    "        current_dim = h_dim\n",
    "    layers.append(nn.Linear(current_dim, 1))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class RnaFmForRegression(nn.Module):\n",
    "    def __init__(self, model_name, head_type='medium'):\n",
    "        super().__init__()\n",
    "        self.rna_fm = RnaFmModel.from_pretrained(model_name)\n",
    "        hidden_size = self.rna_fm.config.hidden_size\n",
    "\n",
    "        print(\"Freezing base model parameters...\")\n",
    "        for param in self.rna_fm.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"Base model parameters frozen.\")\n",
    "\n",
    "        print(f\"Creating regression head of type: {head_type}\")\n",
    "        if head_type == 'simple':\n",
    "            self.regressor = create_simple_regressor(hidden_size)\n",
    "        elif head_type == 'medium':\n",
    "            self.regressor = create_medium_mlp_head(hidden_size)\n",
    "        elif head_type == 'deep':\n",
    "            self.regressor = create_deep_mlp_head(hidden_size)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid head_type. Choose 'simple', 'medium', or 'deep'.\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.rna_fm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        pooled_output = mean_pooling(outputs, attention_mask)\n",
    "\n",
    "        logits = self.regressor(pooled_output)\n",
    "\n",
    "        return logits.squeeze(-1)\n",
    "\n",
    "def initialize_model_and_optimizer(model_name, head_type, learning_rate, weight_decay):\n",
    "    model = RnaFmForRegression(model_name, head_type=head_type)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    print(\"\\nOptimizer will train the following parameters:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(name)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    return model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "EPOCHS = 10\n",
    "HEAD_TYPE = 'medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    mse = mean_squared_error(all_labels, all_preds)\n",
    "    mae = mean_absolute_error(all_labels, all_preds)\n",
    "    r2 = r2_score(all_labels, all_preds)\n",
    "\n",
    "    return avg_loss, mse, mae, r2, all_labels, all_preds\n",
    "\n",
    "def run_training_fold(model, optimizer, criterion, train_loader, val_loader, epochs, device):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    print(f\"Starting training for {epochs} epochs...\")\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_mse, val_mae, val_r2, _, _ = evaluate_model(model, val_loader, criterion, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val MSE: {val_mse:.4f} | Val MAE: {val_mae:.4f} | Val R2: {val_r2:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = copy.deepcopy(model.regressor.state_dict())\n",
    "            print(f\"  -> New best validation loss: {best_val_loss:.4f}. Saving model state.\")\n",
    "\n",
    "        if device == torch.device(\"mps\"):\n",
    "             torch.mps.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "    return best_model_state, train_losses, val_losses, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=SEED)\n",
    "fold_results = []\n",
    "\n",
    "full_dataset = SiRnaDataset(all_sequences, all_labels, tokenizer, MAX_LENGTH)\n",
    "\n",
    "print(f\"\\n--- Starting {NUM_FOLDS}-Fold Cross-Validation ---\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(all_sequences)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{NUM_FOLDS} ---\")\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "    train_loader = DataLoader(full_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "    val_loader = DataLoader(full_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)\n",
    "\n",
    "    print(f\"Train samples: {len(train_idx)}, Validation samples: {len(val_idx)}\")\n",
    "\n",
    "    model, optimizer, criterion = initialize_model_and_optimizer(\n",
    "        MODEL_NAME, HEAD_TYPE, LEARNING_RATE, WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "    best_head_state, train_losses, val_losses, best_val_loss = run_training_fold(\n",
    "        model, optimizer, criterion, train_loader, val_loader, EPOCHS, device\n",
    "    )\n",
    "\n",
    "    if best_head_state:\n",
    "        model.regressor.load_state_dict(best_head_state)\n",
    "        print(\"Loaded best model state for final evaluation.\")\n",
    "    else:\n",
    "        print(\"Warning: No best model state saved (likely only 1 epoch or no improvement). Evaluating last state.\")\n",
    "\n",
    "    final_val_loss, final_mse, final_mae, final_r2, fold_labels, fold_preds = evaluate_model(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "\n",
    "    print(f\"\\nFold {fold+1} Final Validation Metrics:\")\n",
    "    print(f\"  MSE: {final_mse:.4f}\")\n",
    "    print(f\"  MAE: {final_mae:.4f}\")\n",
    "    print(f\"  R2 Score: {final_r2:.4f}\")\n",
    "\n",
    "    fold_results.append({\n",
    "        'fold': fold + 1,\n",
    "        'mse': final_mse,\n",
    "        'mae': final_mae,\n",
    "        'r2': final_r2,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'labels': fold_labels,\n",
    "        'predictions': fold_preds\n",
    "    })\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(range(1, EPOCHS + 1), train_losses, label='Training Loss')\n",
    "    plt.plot(range(1, EPOCHS + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.title(f'Fold {fold+1} - Training & Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    del model, optimizer, criterion, train_loader, val_loader, train_sampler, val_sampler\n",
    "    if device == torch.device(\"mps\"):\n",
    "        torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "print(\"\\n--- Cross-Validation Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(fold_results)\n",
    "print(\"\\n--- Cross-Validation Summary ---\")\n",
    "print(results_df[['fold', 'mse', 'mae', 'r2', 'best_val_loss']])\n",
    "\n",
    "avg_mse = results_df['mse'].mean()\n",
    "std_mse = results_df['mse'].std()\n",
    "avg_mae = results_df['mae'].mean()\n",
    "std_mae = results_df['mae'].std()\n",
    "avg_r2 = results_df['r2'].mean()\n",
    "std_r2 = results_df['r2'].std()\n",
    "\n",
    "print(\"\\nAverage Metrics Across Folds:\")\n",
    "print(f\"  MSE: {avg_mse:.4f} +/- {std_mse:.4f}\")\n",
    "print(f\"  MAE: {avg_mae:.4f} +/- {std_mae:.4f}\")\n",
    "print(f\"  R2 Score: {avg_r2:.4f} +/- {std_r2:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
