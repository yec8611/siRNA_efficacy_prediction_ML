{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from multimolecule import RnaTokenizer, RnaFmModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA for feature extraction\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU for feature extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_NAME = \"multimolecule/rnafm\"\n",
    "MAX_LENGTH = 512\n",
    "EXTRACT_BATCH_SIZE = 16\n",
    "NUM_FOLDS = 5\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = RnaTokenizer.from_pretrained(MODEL_NAME)\n",
    "sep_token = tokenizer.sep_token if tokenizer.sep_token else \"[SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sequences(sirna, mrna):\n",
    "    sirna_str = str(sirna)\n",
    "    mrna_str = str(mrna)\n",
    "    return f\"{sirna_str}{sep_token}{mrna_str}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['combined_sequence'] = df.apply(lambda row: combine_sequences(row['siRNA_sequence'], row['mRNA_sequence']), axis=1)\n",
    "all_sequences = df['combined_sequence'].tolist()\n",
    "all_labels = df['inhibition_value'].to_numpy() \n",
    "\n",
    "print(f\"\\nPrepared {len(all_sequences)} sequences for feature extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "class ExtractionDataset(Dataset):\n",
    "    def __init__(self, sequences, tokenizer, max_len):\n",
    "        self.sequences = sequences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = str(self.sequences[idx])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sequence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        }\n",
    "\n",
    "def extract_rnafm_features(sequences, model, tokenizer, max_len, batch_size, device):\n",
    "    dataset = ExtractionDataset(sequences, tokenizer, max_len)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, sampler=SequentialSampler(dataset))\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    all_features = []\n",
    "    print(f\"Starting feature extraction for {len(sequences)} sequences on device: {device}\")\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "            pooled_output = mean_pooling(outputs, attention_mask)\n",
    "\n",
    "            all_features.append(pooled_output.cpu().numpy())\n",
    "\n",
    "            if (i + 1) % 50 == 0:\n",
    "                 elapsed = time.time() - start_time\n",
    "                 print(f\"  Processed batch {i+1}/{len(data_loader)} ({elapsed:.2f}s)\")\n",
    "\n",
    "    print(f\"Feature extraction completed in {time.time() - start_time:.2f}s\")\n",
    "    features_array = np.concatenate(all_features, axis=0)\n",
    "    print(f\"Extracted features shape: {features_array.shape}\")\n",
    "    return features_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\nLoading RNA-FM model: {MODEL_NAME}\")\n",
    "rnafm_model = RnaFmModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_features = extract_rnafm_features(\n",
    "    all_sequences,\n",
    "    rnafm_model,\n",
    "    tokenizer,\n",
    "    MAX_LENGTH,\n",
    "    EXTRACT_BATCH_SIZE,\n",
    "    device\n",
    ")\n",
    "y_labels = all_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"RandomForest\": RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=20,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        n_jobs=-1,\n",
    "        random_state=SEED\n",
    "    ),\n",
    "\n",
    "    \"XGBoost\": xgb.XGBRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='reg:squarederror',\n",
    "        n_jobs=-1,\n",
    "        random_state=SEED,\n",
    "    ),\n",
    "\n",
    "    \"SVR_RBF\": SVR(\n",
    "        kernel='rbf',\n",
    "        C=10.0,\n",
    "        gamma='scale',\n",
    "        epsilon=0.1\n",
    "    ),\n",
    "\n",
    "    \"LightGBM\": lgb.LGBMRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=7,\n",
    "        num_leaves=31,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='regression',\n",
    "        n_jobs=-1,\n",
    "        random_state=SEED\n",
    "    ),\n",
    "\n",
    "    \"KNeighbors\": KNeighborsRegressor(\n",
    "        n_neighbors=7,\n",
    "        weights='distance',\n",
    "        p=2\n",
    "    ),\n",
    "\n",
    "     \"Ridge\": Ridge(\n",
    "         alpha=1.0,\n",
    "         random_state=SEED\n",
    "     )\n",
    "}\n",
    "\n",
    "print(f\"\\nDefined {len(models)} classical ML models for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=SEED)\n",
    "cv_results = []\n",
    "\n",
    "print(f\"\\n--- Starting {NUM_FOLDS}-Fold Cross-Validation for Classical Models ---\")\n",
    "\n",
    "X = X_features\n",
    "y = y_labels\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{NUM_FOLDS} ---\")\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    for model_name, model_instance in models.items():\n",
    "        print(f\"  Training {model_name}...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('regressor', model_instance)\n",
    "        ])\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"    {model_name} Fold {fold+1} | MSE: {mse:.4f} | MAE: {mae:.4f} | R2: {r2:.4f} | Time: {elapsed:.2f}s\")\n",
    "\n",
    "        cv_results.append({\n",
    "            'fold': fold + 1,\n",
    "            'model': model_name,\n",
    "            'mse': mse,\n",
    "            'mae': mae,\n",
    "            'r2': r2\n",
    "        })\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n--- Cross-Validation Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(cv_results)\n",
    "\n",
    "print(\"\\n--- Cross-Validation Summary (Classical Models on RNA-FM Features) ---\")\n",
    "\n",
    "summary = results_df.groupby('model').agg(\n",
    "    avg_mse=('mse', 'mean'),\n",
    "    std_mse=('mse', 'std'),\n",
    "    avg_mae=('mae', 'mean'),\n",
    "    std_mae=('mae', 'std'),\n",
    "    avg_r2=('r2', 'mean'),\n",
    "    std_r2=('r2', 'std')\n",
    ").reset_index()\n",
    "\n",
    "summary['MSE'] = summary.apply(lambda row: f\"{row['avg_mse']:.4f} +/- {row['std_mse']:.4f}\", axis=1)\n",
    "summary['MAE'] = summary.apply(lambda row: f\"{row['avg_mae']:.4f} +/- {row['std_mae']:.4f}\", axis=1)\n",
    "summary['R2'] = summary.apply(lambda row: f\"{row['avg_r2']:.4f} +/- {row['std_r2']:.4f}\", axis=1)\n",
    "\n",
    "print(summary[['model', 'MSE', 'MAE', 'R2']].to_string(index=False))\n",
    "\n",
    "best_model_r2 = summary.loc[summary['avg_r2'].idxmax()]\n",
    "print(f\"\\nBest model based on average R2 Score: {best_model_r2['model']} (R2 = {best_model_r2['R2']})\")\n",
    "best_model_mse = summary.loc[summary['avg_mse'].idxmin()]\n",
    "print(f\"Best model based on average MSE: {best_model_mse['model']} (MSE = {best_model_mse['MSE']})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
